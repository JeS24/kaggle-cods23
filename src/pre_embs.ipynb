{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To generate pre_embeddings for combined `train.csv` & `test.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd, numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset - NOTE: Train data\n",
    "data_path = \"../data/kagdata/train.csv\"\n",
    "data = pd.read_csv(data_path)\n",
    "\n",
    "# NOTE: Test data\n",
    "tdata_path = \"../data/kagdata/test.csv\"\n",
    "tdata = pd.read_csv(tdata_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop(columns=['label', 'pre requisite taxonomy', 'concept taxonomy']) # Removing these columns (all strings except label)\n",
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdata = tdata.drop(columns=['ID', 'pre requisite taxonomy', 'concept taxonomy']) # Removing these columns (all strings except label)\n",
    "# tdata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* There are some common entries between \"concept\" & \"pre requisite\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: Checking if any of the concepts are in the pre-requisites & vice versa\n",
    "cons = data['concept'].unique() # 666 concepts\n",
    "prs = data['pre requisite'].unique() # 439 pre requisites\n",
    "\n",
    "# NOTE: Some common entries between \"concept\" & \"pre requisite\"\n",
    "assert not np.all([c not in prs for c in cons])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: Checking if any of the concepts are in the pre-requisites & vice versa\n",
    "tcons = tdata['concept'].unique() # 475 concepts\n",
    "tprs = tdata['pre requisite'].unique() # 461 pre requisites\n",
    "\n",
    "# NOTE: Some common entries between \"concept\" & \"pre requisite\"\n",
    "assert not np.all([c not in tprs for c in tcons])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training\tTesting\n",
      "(46,) \t\t (208,)\n"
     ]
    }
   ],
   "source": [
    "# NOTE: There are edges (entries) with permuted concepts & pre-requisites\n",
    "def get_rev_edge_count(dat):\n",
    "    \"\"\"\n",
    "    Returns the number of permuted edges in train.csv / test.csv\n",
    "\n",
    "    \"\"\"\n",
    "    arr1 = dat[['concept', 'pre requisite']].values.astype(str)\n",
    "    arr2 = dat[['pre requisite', 'concept']].values.astype(str)\n",
    "\n",
    "    # https://stackoverflow.com/a/67113105/11922029\n",
    "    m = (arr1[:, None] == arr2).all(-1).any(1)\n",
    "    rev_counts = np.where(m)[0].shape\n",
    "\n",
    "    return rev_counts\n",
    "\n",
    "print(\"Training\\tTesting\")\n",
    "print(get_rev_edge_count(data), \"\\t\\t\", get_rev_edge_count(tdata)) # 46, 208 NOTE: number of entries with permuted videos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merging the two dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3411, 983)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dat = pd.concat([data, tdata], axis=0, ignore_index=True)\n",
    "dat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: Merging dataframes and saving to disk - Used later to get node indices in inds_lbls.ipynb \n",
    "# dat.to_csv(\"../data/kagdata/combined.csv\", index=False) # NOTE: Careful about overwriting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Test `dat` in the same manner as `data` (train) & `tdata` (test) above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(262,)\n"
     ]
    }
   ],
   "source": [
    "# NOTE: Checking if any of the concepts are in the pre-requisites & vice versa\n",
    "cons = dat['concept'].unique() # 881 concepts\n",
    "prs = dat['pre requisite'].unique() # 720 pre requisites\n",
    "\n",
    "# NOTE: Some common entries between \"concept\" & \"pre requisite\"\n",
    "assert not np.all([c not in prs for c in cons])\n",
    "\n",
    "# NOTE: Some common entries between \"concept\" & \"pre requisite\"\n",
    "print(get_rev_edge_count(dat)) # 262 = 46 + 208 + 8 NOTE: number of entries with permuted videos. There are 8 permuted edges between train.csv & test.csv."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For each concept (fixed), aggregate over pre requisites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1092, (1092,))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shared = np.intersect1d(cons, prs) # 509\n",
    "cons_uniq = np.setdiff1d(cons, shared) # 372\n",
    "prs_uniq = np.setdiff1d(prs, shared) # 211\n",
    "\n",
    "overall = np.concatenate([cons_uniq, prs_uniq, shared])\n",
    "sum((cons_uniq.shape[0], prs_uniq.shape[0], shared.shape[0])), overall.shape\n",
    "# NOTE: 1 entry in metadata is not used at all in train.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_pre_embed = []\n",
    "for title in cons_uniq:\n",
    "    # Simple summed aggregate over all \"pre requisites\" for this \"concept\"\n",
    "    pre_embed = dat[dat['concept'] == title].drop(columns=['concept', 'pre requisite']).sum(axis=0)\n",
    "\n",
    "    title_pre_embed.append([title, list(pre_embed.values)])\n",
    "\n",
    "for title in prs_uniq:\n",
    "    # Simple summed aggregate over all \"concepts\" for this \"pre requisite\"\n",
    "    pre_embed = dat[dat['pre requisite'] == title].drop(columns=['concept', 'pre requisite']).sum(axis=0)\n",
    "\n",
    "    title_pre_embed.append([title, list(pre_embed.values)])\n",
    "\n",
    "for title in shared:\n",
    "    # Simple summed aggregate over all \"concepts\" for this \"pre requisite\"\n",
    "    con_pre_embed = dat[dat['concept'] == title].drop(columns=['concept', 'pre requisite']).sum(axis=0)\n",
    "    # Simple summed aggregate over all \"pre requisites\" for this \"concept\"\n",
    "    pr_pre_embed = dat[dat['pre requisite'] == title].drop(columns=['concept', 'pre requisite']).sum(axis=0)\n",
    "\n",
    "    title_pre_embed.append([title, list((con_pre_embed + pr_pre_embed).values)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_embeddings = pd.DataFrame(title_pre_embed)\n",
    "pre_embeddings.columns = ['title', 'pre_embed_vec']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: Scaling\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "# scaler = MinMaxScaler()\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Run scaler on each row in pre_embeddings['pre_embed_vec']\n",
    "pre_embeddings['pre_embed_vec'] = pre_embeddings['pre_embed_vec'].apply(lambda x: scaler.fit_transform(np.array(x).reshape(-1, 1)))\n",
    "\n",
    "# Convert back to list\n",
    "pre_embeddings['pre_embed_vec'] = pre_embeddings['pre_embed_vec'].apply(lambda x: x.reshape(-1).tolist())\n",
    "\n",
    "# Save to csv | NOTE: 981-length un/scaled vector for each title\n",
    "# pre_embeddings.to_csv(\"../data/kagdata/pre_embeddings_std_combined.csv\", index=False) # NOTE: Careful about overwriting"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "reqv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
